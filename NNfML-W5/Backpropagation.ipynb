{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of [iPyMacLern](http://ekaakurniawan.github.io/iPyMacLern/) project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Copyright (C) 2016 by Eka A. Kurniawan\n",
    "> \n",
    "> eka.a.kurniawan(ta)gmail(tod)com\n",
    "> \n",
    "> This program is free software: you can redistribute it and/or modify\n",
    "> it under the terms of the GNU General Public License as published by\n",
    "> the Free Software Foundation, either version 3 of the License, or\n",
    "> (at your option) any later version.\n",
    "> \n",
    "> This program is distributed in the hope that it will be useful,\n",
    "> but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "> MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "> GNU General Public License for more details.\n",
    "> \n",
    "> You should have received a copy of the GNU General Public License\n",
    "> along with this program.  If not, see <http://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Display graph inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Display graph in 'retina' format for Mac with retina display. Others, use PNG or SVG format.\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#%config InlineBackend.figure_format = 'PNG'\n",
    "#%config InlineBackend.figure_format = 'SVG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tested On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.5.2 (default, Jun 27 2016, 03:10:38) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 7.0.2 (clang-700.1.81)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python %s\" % sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy 1.11.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"NumPy %s\" % np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SciPy 0.18.0\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import scipy.io as sio\n",
    "print(\"SciPy %s\" % scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Dataset](#Dataset)\n",
    "- [Training](#Training)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset from MATLAB formated dataset.$^{[1]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_dataset(N):\n",
    "    dataset = sio.loadmat('data.mat')\n",
    "    struct = dataset['data']\n",
    "    data = struct[0,0]\n",
    "    \n",
    "    print(\"Dataset Info\")\n",
    "    print(\"------------\")\n",
    "    \n",
    "    num_dimensions = data['trainData'].shape[0]\n",
    "    print(\"Number dimensions                        :%18s\" % num_dimensions)\n",
    "    D = num_dimensions - 1\n",
    "    print(\"Number input dimensions                  :%18s\" % D)\n",
    "    \n",
    "    # Get training data\n",
    "    print(\"Number training data                     :%18s\" % data['trainData'].shape[1])\n",
    "    print(\"Mini batches size                        :%18s\" % N)\n",
    "    M = int(data['trainData'].shape[1] / N)\n",
    "    print(\"Number mini batches                      :%18s\" % M)\n",
    "    \n",
    "    training_input = data['trainData'][:D, :(N*M)].reshape((D, M, N)) - 1\n",
    "    print(\"Training input shape                     :%18s\" % str(training_input.shape))\n",
    "    training_target = data['trainData'][D, :(N*M)].reshape(1, M, N) - 1\n",
    "    print(\"Training target shape                    :%18s\" % str(training_target.shape))\n",
    "    \n",
    "    # Get validation data\n",
    "    validation_input = data['validData'][:D, :] - 1\n",
    "    print(\"Validation input shape                   :%18s\" % str(validation_input.shape))\n",
    "    validation_target = data['validData'][D, :] - 1\n",
    "    print(\"Validation target shape                  :%18s\" % str(validation_target.shape))\n",
    "    \n",
    "    # Get testing data\n",
    "    testing_input = data['testData'][:D, :] - 1\n",
    "    print(\"Testing input shape                      :%18s\" % str(testing_input.shape))\n",
    "    testing_target = data['testData'][D, :] - 1\n",
    "    print(\"Testing target shape                     :%18s\" % str(testing_target.shape))\n",
    "    \n",
    "    # Get vocabulary\n",
    "    vocabulary = data['vocab'][0]\n",
    "    print(\"Vocabulary size                          :%18s\" % len(vocabulary))\n",
    "    \n",
    "    return training_input, training_target, validation_input, validation_target, \\\n",
    "           testing_input, testing_target, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info\n",
      "------------\n",
      "Number dimensions                        :                 4\n",
      "Number input dimensions                  :                 3\n",
      "Number training data                     :            372550\n",
      "Mini batches size                        :               100\n",
      "Number mini batches                      :              3725\n",
      "Training input shape                     :    (3, 3725, 100)\n",
      "Training target shape                    :    (1, 3725, 100)\n",
      "Validation input shape                   :        (3, 46568)\n",
      "Validation target shape                  :          (46568,)\n",
      "Testing input shape                      :        (3, 46568)\n",
      "Testing target shape                     :          (46568,)\n",
      "Vocabulary size                          :               250\n"
     ]
    }
   ],
   "source": [
    "training_input, training_target, validation_input, validation_target, \\\n",
    "    testing_input, testing_target, vocabulary = read_dataset(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_forwardpropagation(input_batch, \\\n",
    "                               embedding_layer_weights, hidden_layer_weights, output_layer_weights, \\\n",
    "                               hidden_layer_bias, output_layer_bias):\n",
    "    \n",
    "    # Setup neural network parameters\n",
    "    num_words, mini_batch_size = input_batch.shape\n",
    "    vocabulary_size, embedding_layer_size = embedding_layer_weights.shape\n",
    "    hidden_layer_size = hidden_layer_weights.shape[1]\n",
    "    \n",
    "    # Compute embedding layer state\n",
    "    embedding_layer_state = np.transpose( \\\n",
    "        embedding_layer_weights[ \\\n",
    "            input_batch.reshape(num_words * mini_batch_size)]).reshape( \\\n",
    "                embedding_layer_size * num_words, mini_batch_size)\n",
    "    \n",
    "    # Compute hidden layer state\n",
    "    hidden_layer_state = 1 / (1 + np.exp(-np.dot( \\\n",
    "                np.transpose(hidden_layer_weights), embedding_layer_state) + hidden_layer_bias))\n",
    "\n",
    "    # Compute output layer state\n",
    "    inputs_to_softmax = np.dot(np.transpose(output_layer_weights), hidden_layer_state) + output_layer_bias\n",
    "    max_inputs_to_softmax = np.max(inputs_to_softmax, axis=0)\n",
    "    inputs_to_softmax = inputs_to_softmax - max_inputs_to_softmax\n",
    "    output_layer_state = np.exp(inputs_to_softmax)\n",
    "    output_layer_state = output_layer_state / np.sum(output_layer_state, axis=0)\n",
    "    \n",
    "    return embedding_layer_state, hidden_layer_state, output_layer_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def perform_training(num_epochs, mini_batch_size):\n",
    "    tic = time.time()\n",
    "    \n",
    "    # Setup neural network and learning parameters\n",
    "    lmda = 0.1                      # learning rate\n",
    "    momentum = 0.9\n",
    "    embedding_layer_size = 50\n",
    "    hidden_layer_size = 200\n",
    "    init_sigma = 0.01               # standard deviation of initial weight\n",
    "    \n",
    "    # Setup display parameters\n",
    "    training_interval = 100\n",
    "    validation_interval = 1000\n",
    "\n",
    "    # Read dataset\n",
    "    training_input, training_target, validation_input, validation_target, \\\n",
    "        testing_input, testing_target, vocabulary = read_dataset(mini_batch_size)\n",
    "\n",
    "    # Setup neural network data\n",
    "    print(\"\")\n",
    "    print(\"Neural Network Info\")\n",
    "    print(\"-------------------\")\n",
    "        \n",
    "    [input_layer_size, num_mini_batches, mini_batch_size] = training_input.shape\n",
    "    output_layer_size = vocabulary.shape[0]\n",
    "\n",
    "    # Setup embedding layer\n",
    "    embedding_layer_weights = init_sigma * np.random.randn(output_layer_size, embedding_layer_size)\n",
    "    print(\"Embedding layer weights shape            :%18s\" % str(embedding_layer_weights.shape))\n",
    "    embedding_layer_weights_delta = np.zeros([output_layer_size, embedding_layer_size])\n",
    "    print(\"Embedding layer weights delta shape      :%18s\" % str(embedding_layer_weights_delta.shape))\n",
    "    embedding_layer_weights_gradient = np.zeros([output_layer_size, embedding_layer_size])\n",
    "    print(\"Embedding layer weights gradient shape   :%18s\" % str(embedding_layer_weights_gradient.shape))\n",
    "    \n",
    "    # Setup hidden layer\n",
    "    hidden_layer_weights = init_sigma * np.random.randn(input_layer_size * embedding_layer_size, hidden_layer_size)\n",
    "    print(\"Hidden layer weights shape               :%18s\" % str(hidden_layer_weights.shape))\n",
    "    hidden_layer_weights_delta = np.zeros([input_layer_size * embedding_layer_size, hidden_layer_size])\n",
    "    print(\"Hidden layer weights delta shape         :%18s\" % str(hidden_layer_weights_delta.shape))\n",
    "    hidden_layer_bias = np.zeros((hidden_layer_size, 1))\n",
    "    print(\"Hidden layer bias shape                  :%18s\" % str(hidden_layer_bias.shape))\n",
    "    hidden_layer_bias_delta = np.zeros((hidden_layer_size, 1))\n",
    "    print(\"Hidden layer bias delta shape            :%18s\" % str(hidden_layer_bias_delta.shape))\n",
    "\n",
    "    # Setup output layer\n",
    "    output_layer_weights = init_sigma * np.random.randn(hidden_layer_size, output_layer_size)\n",
    "    print(\"Output layer weights shape               :%18s\" % str(output_layer_weights.shape))\n",
    "    output_layer_weights_delta = np.zeros([hidden_layer_size, output_layer_size])\n",
    "    print(\"Output layer weights delta shape         :%18s\" % str(output_layer_weights_delta.shape))\n",
    "    output_layer_bias = np.zeros((output_layer_size, 1))\n",
    "    print(\"Output layer bias shape                  :%18s\" % str(output_layer_bias.shape))\n",
    "    output_layer_bias_delta = np.zeros((output_layer_size, 1))\n",
    "    print(\"Output layer bias delta shape            :%18s\" % str(output_layer_bias_delta.shape))\n",
    "    \n",
    "    expansion = np.eye(output_layer_size)\n",
    "    print(\"Expansion shape                          :%18s\" % str(expansion.shape))\n",
    "\n",
    "    # Training epoch\n",
    "    count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"Epoch\", epoch + 1)\n",
    "        this_chunk_CE = 0\n",
    "        trainset_CE = 0\n",
    "        \n",
    "        # Training mini batch\n",
    "        for m in range(num_mini_batches):\n",
    "            \n",
    "            # Perform forwardpropagation\n",
    "            # --------------------------\n",
    "            training_input_batch = training_input[:, m, :]\n",
    "            embedding_layer_state, hidden_layer_state, output_layer_state = \\\n",
    "                perform_forwardpropagation(training_input_batch, \\\n",
    "                                           embedding_layer_weights, hidden_layer_weights, output_layer_weights, \\\n",
    "                                           hidden_layer_bias, output_layer_bias)\n",
    "\n",
    "            # Compute cost\n",
    "            training_target_batch = training_target[:, m, :][0]\n",
    "            expanded_training_target_batch = expansion[:, training_target_batch]\n",
    "            error_derivative = output_layer_state - expanded_training_target_batch\n",
    "\n",
    "            # Compute cross entrophy (CE)\n",
    "            CE = -np.sum(expanded_training_target_batch * np.log(output_layer_state)) / mini_batch_size\n",
    "\n",
    "            # Display cross entrophy (CE)\n",
    "            count = count + 1\n",
    "            this_chunk_CE = this_chunk_CE + ((CE - this_chunk_CE) / count)\n",
    "            if (m % training_interval) == 0:\n",
    "                print(\"Batch %5d Training CE %3.3f\" % (m, this_chunk_CE))\n",
    "                count = 0\n",
    "                this_chunk_CE = 0\n",
    "            trainset_CE = trainset_CE + ((CE - trainset_CE) / (m + 1));\n",
    "            \n",
    "            # Perform backpropagation\n",
    "            # -----------------------\n",
    "\n",
    "            # Output layer backpropagation\n",
    "            output_layer_weights_gradient = np.dot(hidden_layer_state, np.transpose(error_derivative))\n",
    "            output_layer_bias_gradient = error_derivative.sum(axis=1, keepdims=True)\n",
    "            backpropagation_derivative_1 = np.dot(output_layer_weights, error_derivative) * \\\n",
    "                hidden_layer_state * (1 - hidden_layer_state)\n",
    "\n",
    "            # Hidden layer backpropagation\n",
    "            hidden_layer_weights_gradient = np.dot(embedding_layer_state, np.transpose(backpropagation_derivative_1))\n",
    "            hidden_layer_bias_gradient = backpropagation_derivative_1.sum(axis=1, keepdims=True)\n",
    "            backpropagation_derivative_2 = np.dot(hidden_layer_weights, backpropagation_derivative_1)\n",
    "\n",
    "            # Update embedding layer weights\n",
    "            embedding_layer_weights_gradient[:] = 0\n",
    "            for w in range(input_layer_size):\n",
    "                embedding_layer_weights_gradient = embedding_layer_weights_gradient + \\\n",
    "                    np.dot(expansion[:, training_input_batch[w, :]], \\\n",
    "                           np.transpose(backpropagation_derivative_2[w*embedding_layer_size : \\\n",
    "                                                                     ((w+1)*embedding_layer_size), :]))\n",
    "            embedding_layer_weights_delta = (momentum * embedding_layer_weights_delta) + \\\n",
    "                (embedding_layer_weights_gradient / mini_batch_size)\n",
    "            embedding_layer_weights = embedding_layer_weights - (lmda * embedding_layer_weights_delta)\n",
    "\n",
    "            # Update hidden layer weights\n",
    "            hidden_layer_weights_delta = (momentum * hidden_layer_weights_delta) + \\\n",
    "                (hidden_layer_weights_gradient / mini_batch_size)\n",
    "            hidden_layer_weights = hidden_layer_weights - (lmda * hidden_layer_weights_delta)\n",
    "\n",
    "            # Update output layer weights\n",
    "            output_layer_weights_delta = (momentum * output_layer_weights_delta) + \\\n",
    "                (output_layer_weights_gradient / mini_batch_size)\n",
    "            output_layer_weights = output_layer_weights - (lmda * output_layer_weights_delta)\n",
    "\n",
    "            # Update hidden layer bias\n",
    "            hidden_layer_bias_delta = (momentum * hidden_layer_bias_delta) + \\\n",
    "                (hidden_layer_bias_gradient / mini_batch_size)\n",
    "            hidden_layer_bias = hidden_layer_bias - (lmda * hidden_layer_bias_delta)\n",
    "\n",
    "            # Update output layer bias\n",
    "            output_layer_bias_delta = (momentum * output_layer_bias_delta) + \\\n",
    "                (output_layer_bias_gradient / mini_batch_size)\n",
    "            output_layer_bias = output_layer_bias - (lmda * output_layer_bias_delta)\n",
    "            \n",
    "            # Perform validation\n",
    "            # ------------------\n",
    "            if (m % validation_interval) == 0:\n",
    "                embedding_layer_state, hidden_layer_state, output_layer_state = \\\n",
    "                    perform_forwardpropagation(validation_input, \\\n",
    "                                               embedding_layer_weights, hidden_layer_weights, output_layer_weights, \\\n",
    "                                               hidden_layer_bias, output_layer_bias)\n",
    "                validation_data_size = validation_input.shape[1]\n",
    "                expanded_validation_target = expansion[:, validation_target]\n",
    "                CE = -np.sum(expanded_validation_target * np.log(output_layer_state)) / validation_data_size\n",
    "                print(\"############################# Validation CE %.3f\" % CE)\n",
    "            \n",
    "            \n",
    "    # Perform testing\n",
    "    # ---------------\n",
    "    embedding_layer_state, hidden_layer_state, output_layer_state = \\\n",
    "        perform_forwardpropagation(testing_input, \\\n",
    "                                   embedding_layer_weights, hidden_layer_weights, output_layer_weights, \\\n",
    "                                   hidden_layer_bias, output_layer_bias)\n",
    "    testing_data_size = testing_input.shape[1]\n",
    "    expanded_test_target = expansion[:, testing_target]\n",
    "    CE = -np.sum(expanded_test_target * np.log(output_layer_state)) / testing_data_size\n",
    "    print(\"\")\n",
    "    print(\"#############################\")\n",
    "    print(\"Final testing CE %.3f\" % CE)\n",
    "    print(\"#############################\")\n",
    "    \n",
    "    toc = time.time()\n",
    "    print(\"\")\n",
    "    print(\"Total runtime %.2f seconds\" % (toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info\n",
      "------------\n",
      "Number dimensions                        :                 4\n",
      "Number input dimensions                  :                 3\n",
      "Number training data                     :            372550\n",
      "Mini batches size                        :               100\n",
      "Number mini batches                      :              3725\n",
      "Training input shape                     :    (3, 3725, 100)\n",
      "Training target shape                    :    (1, 3725, 100)\n",
      "Validation input shape                   :        (3, 46568)\n",
      "Validation target shape                  :          (46568,)\n",
      "Testing input shape                      :        (3, 46568)\n",
      "Testing target shape                     :          (46568,)\n",
      "Vocabulary size                          :               250\n",
      "\n",
      "Neural Network Info\n",
      "-------------------\n",
      "Embedding layer weights shape            :         (250, 50)\n",
      "Embedding layer weights delta shape      :         (250, 50)\n",
      "Embedding layer weights gradient shape   :         (250, 50)\n",
      "Hidden layer weights shape               :        (150, 200)\n",
      "Hidden layer weights delta shape         :        (150, 200)\n",
      "Hidden layer bias shape                  :          (200, 1)\n",
      "Hidden layer bias delta shape            :          (200, 1)\n",
      "Output layer weights shape               :        (200, 250)\n",
      "Output layer weights delta shape         :        (200, 250)\n",
      "Output layer bias shape                  :          (250, 1)\n",
      "Output layer bias delta shape            :          (250, 1)\n",
      "Expansion shape                          :        (250, 250)\n",
      "\n",
      "Epoch 1\n",
      "Batch     0 Training CE 5.511\n",
      "############################# Validation CE 5.338\n",
      "Batch   100 Training CE 4.597\n",
      "Batch   200 Training CE 4.533\n",
      "Batch   300 Training CE 4.508\n",
      "Batch   400 Training CE 4.533\n",
      "Batch   500 Training CE 4.516\n",
      "Batch   600 Training CE 4.569\n",
      "Batch   700 Training CE 4.581\n",
      "Batch   800 Training CE 4.541\n",
      "Batch   900 Training CE 4.633\n",
      "Batch  1000 Training CE 4.595\n",
      "############################# Validation CE 4.575\n",
      "Batch  1100 Training CE 4.609\n",
      "Batch  1200 Training CE 4.677\n",
      "Batch  1300 Training CE 4.661\n",
      "Batch  1400 Training CE 4.689\n",
      "Batch  1500 Training CE 4.638\n",
      "Batch  1600 Training CE 4.700\n",
      "Batch  1700 Training CE 4.550\n",
      "Batch  1800 Training CE 4.462\n",
      "Batch  1900 Training CE 4.378\n",
      "Batch  2000 Training CE 4.316\n",
      "############################# Validation CE 4.387\n",
      "Batch  2100 Training CE 4.316\n",
      "Batch  2200 Training CE 4.283\n",
      "Batch  2300 Training CE 4.285\n",
      "Batch  2400 Training CE 4.175\n",
      "Batch  2500 Training CE 4.100\n",
      "Batch  2600 Training CE 4.077\n",
      "Batch  2700 Training CE 4.011\n",
      "Batch  2800 Training CE 3.905\n",
      "Batch  2900 Training CE 3.860\n",
      "Batch  3000 Training CE 3.714\n",
      "############################# Validation CE 3.750\n",
      "Batch  3100 Training CE 3.692\n",
      "Batch  3200 Training CE 3.639\n",
      "Batch  3300 Training CE 3.550\n",
      "Batch  3400 Training CE 3.529\n",
      "Batch  3500 Training CE 3.473\n",
      "Batch  3600 Training CE 3.478\n",
      "Batch  3700 Training CE 3.374\n",
      "\n",
      "Epoch 2\n",
      "Batch     0 Training CE 0.137\n",
      "############################# Validation CE 3.378\n",
      "Batch   100 Training CE 3.390\n",
      "Batch   200 Training CE 3.382\n",
      "Batch   300 Training CE 3.328\n",
      "Batch   400 Training CE 3.302\n",
      "Batch   500 Training CE 3.259\n",
      "Batch   600 Training CE 3.291\n",
      "Batch   700 Training CE 3.272\n",
      "Batch   800 Training CE 3.217\n",
      "Batch   900 Training CE 3.209\n",
      "Batch  1000 Training CE 3.213\n",
      "############################# Validation CE 3.185\n",
      "Batch  1100 Training CE 3.166\n",
      "Batch  1200 Training CE 3.187\n",
      "Batch  1300 Training CE 3.141\n",
      "Batch  1400 Training CE 3.175\n",
      "Batch  1500 Training CE 3.159\n",
      "Batch  1600 Training CE 3.132\n",
      "Batch  1700 Training CE 3.115\n",
      "Batch  1800 Training CE 3.126\n",
      "Batch  1900 Training CE 3.091\n",
      "Batch  2000 Training CE 3.062\n",
      "############################# Validation CE 3.104\n",
      "Batch  2100 Training CE 3.091\n",
      "Batch  2200 Training CE 3.082\n",
      "Batch  2300 Training CE 3.047\n",
      "Batch  2400 Training CE 3.055\n",
      "Batch  2500 Training CE 3.050\n",
      "Batch  2600 Training CE 3.071\n",
      "Batch  2700 Training CE 3.077\n",
      "Batch  2800 Training CE 3.036\n",
      "Batch  2900 Training CE 3.038\n",
      "Batch  3000 Training CE 2.989\n",
      "############################# Validation CE 3.001\n",
      "Batch  3100 Training CE 2.997\n",
      "Batch  3200 Training CE 2.972\n",
      "Batch  3300 Training CE 2.958\n",
      "Batch  3400 Training CE 3.003\n",
      "Batch  3500 Training CE 2.977\n",
      "Batch  3600 Training CE 2.983\n",
      "Batch  3700 Training CE 2.943\n",
      "\n",
      "#############################\n",
      "Final testing CE 2.958\n",
      "#############################\n",
      "\n",
      "Total runtime 43.07 seconds\n"
     ]
    }
   ],
   "source": [
    "perform_training(num_epochs=2, mini_batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. G. Hinton, 2016. _Neural Networks for Machine Learning_. Week 5 Programming Assignment 2: Learning Word Representations. University of Toronto. Coursera. https://www.coursera.org/learn/neural-networks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
